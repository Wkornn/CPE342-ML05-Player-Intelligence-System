{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player Segment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle catboost optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    COLAB_ENV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "N_TRIALS = 10\n",
    "TEST_SIZE = 0.1\n",
    "CV_FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_data():\n",
    "    \"\"\"Download and extract Kaggle competition data.\"\"\"\n",
    "    if COLAB_ENV:\n",
    "        try:\n",
    "            uploaded = files.upload()\n",
    "        except Exception as e:\n",
    "            print(f\"File upload failed: {e}\")\n",
    "    else:\n",
    "        print(\"Running outside of Colab. Ensure kaggle.json is in ~/.kaggle/\")\n",
    "\n",
    "    if 'kaggle.json' in os.listdir('.'):\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !mv kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "    else:\n",
    "        print(\"kaggle.json not found.\")\n",
    "\n",
    "    if not os.path.exists('cpe342-karena.zip'):\n",
    "        print(\"Downloading data...\")\n",
    "        !kaggle competitions download -c cpe342-karena\n",
    "    else:\n",
    "        print(\"Data already downloaded.\")\n",
    "\n",
    "    if os.path.exists('cpe342-karena.zip'):\n",
    "        print(\"Extracting data...\")\n",
    "        with zipfile.ZipFile('cpe342-karena.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(\"Data extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, is_training=True):\n",
    "    \"\"\"Load and preprocess player data.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if is_training:\n",
    "        df = df.drop(['id', 'player_id'], axis=1)\n",
    "    else:\n",
    "        ids = df['id'].copy()\n",
    "        df = df.drop(['id', 'player_id'], axis=1)\n",
    "    \n",
    "    continuous_features = [\n",
    "        'play_frequency', 'avg_session_duration', 'total_playtime_hours',\n",
    "        'login_streak', 'days_since_last_login', 'total_spending_thb',\n",
    "        'avg_monthly_spending', 'spending_frequency', 'friend_count',\n",
    "        'team_play_percentage', 'chat_activity_score', 'friend_invites_sent',\n",
    "        'gifts_sent_received', 'ranked_participation_rate', 'tournament_entries',\n",
    "        'competitive_rank', 'win_rate_ranked', 'watches_esports',\n",
    "        'achievement_completion_rate', 'collection_progress', 'rare_items_count',\n",
    "        'speed_of_progression', 'item_type_preference_cosmetic',\n",
    "        'item_type_preference_performance', 'item_type_preference_social',\n",
    "        'account_age_days', 'vip_tier', 'responds_to_discounts',\n",
    "        'preferred_game_mode', 'avg_match_length', 'peak_concurrent_hours',\n",
    "        'random_metric_1', 'random_metric_2', 'random_metric_3'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'region', 'platform', 'device_type', 'payment_method',\n",
    "        'language', 'account_status', 'player_type_tag',\n",
    "        'engagement_level', 'loyalty_tier', 'skill_tier'\n",
    "    ]\n",
    "    \n",
    "    # Impute continuous features\n",
    "    missing_cont = df[continuous_features].isnull().mean() * 100\n",
    "    cont_to_impute = missing_cont[missing_cont < 30].index.tolist()\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[cont_to_impute] = imputer.fit_transform(df[cont_to_impute])\n",
    "    \n",
    "    # Impute categorical features\n",
    "    for col in categorical_features:\n",
    "        missing_pct = df[col].isnull().mean() * 100\n",
    "        if missing_pct < 20:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    if is_training:\n",
    "        return df\n",
    "    else:\n",
    "        return df, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Create derived features and encode categorical variables.\"\"\"\n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    categorical_features = [\n",
    "        'region', 'platform', 'device_type', 'payment_method',\n",
    "        'language', 'account_status', 'player_type_tag',\n",
    "        'engagement_level', 'loyalty_tier', 'skill_tier'\n",
    "    ]\n",
    "    \n",
    "    # Time & Play Patterns\n",
    "    df_fe['freq_per_day'] = df_fe['play_frequency'] / df_fe['account_age_days']\n",
    "    df_fe['avg_session_per_play'] = df_fe['avg_session_duration'] / df_fe['play_frequency']\n",
    "    df_fe['playtime_per_login'] = df_fe['total_playtime_hours'] / df_fe['login_streak']\n",
    "    df_fe['consistency_score'] = df_fe['login_streak'] / df_fe['account_age_days']\n",
    "    \n",
    "    # Spending Behavior\n",
    "    df_fe['total_avg_ratio'] = df_fe['total_spending_thb'] / (df_fe['avg_monthly_spending'] + 1e-6)\n",
    "    df_fe['spend_per_playtime'] = df_fe['total_spending_thb'] / (df_fe['total_playtime_hours'] + 1e-6)\n",
    "    df_fe['spending_per_freq'] = df_fe['spending_frequency'] / (df_fe['play_frequency'] + 1e-6)\n",
    "    df_fe['discount_effect'] = df_fe['responds_to_discounts'] * df_fe['total_spending_thb']\n",
    "    \n",
    "    # Social & Team Engagement\n",
    "    df_fe['friends_per_play'] = df_fe['friend_count'] / (df_fe['play_frequency'] + 1e-6)\n",
    "    df_fe['social_score'] = df_fe['friend_invites_sent'] + df_fe['gifts_sent_received']\n",
    "    df_fe['teamplay_ratio'] = df_fe['team_play_percentage'] / 100\n",
    "    df_fe['ranked_per_hour'] = df_fe['ranked_participation_rate'] / (df_fe['total_playtime_hours'] + 1e-6)\n",
    "    df_fe['tournament_per_hour'] = df_fe['tournament_entries'] / (df_fe['total_playtime_hours'] + 1e-6)\n",
    "    \n",
    "    # Progression & Achievement\n",
    "    df_fe['achievements_per_hour'] = df_fe['achievement_completion_rate'] / (df_fe['total_playtime_hours'] + 1e-6)\n",
    "    df_fe['collection_per_day'] = df_fe['collection_progress'] / df_fe['account_age_days']\n",
    "    df_fe['rare_items_per_playtime'] = df_fe['rare_items_count'] / (df_fe['total_playtime_hours'] + 1e-6)\n",
    "    df_fe['progress_speed_per_session'] = df_fe['speed_of_progression'] / (df_fe['avg_session_duration'] + 1e-6)\n",
    "    \n",
    "    # Engagement Intensity\n",
    "    df_fe['playtime_per_day'] = df_fe['total_playtime_hours'] / df_fe['account_age_days']\n",
    "    df_fe['avg_match_hours'] = df_fe['avg_match_length'] * df_fe['play_frequency']\n",
    "    df_fe['peak_intensity'] = df_fe['peak_concurrent_hours'] / (df_fe['avg_session_duration'] + 1e-6)\n",
    "    \n",
    "    # Interaction Features\n",
    "    df_fe['spend_vip'] = df_fe['total_spending_thb'] * df_fe['vip_tier']\n",
    "    df_fe['friends_team_interaction'] = df_fe['friend_count'] * df_fe['team_play_percentage']\n",
    "    df_fe['achievements_collection'] = df_fe['achievement_completion_rate'] * df_fe['collection_progress']\n",
    "    \n",
    "    # Handle categorical features\n",
    "    df_fe = pd.get_dummies(df_fe, columns=categorical_features, dummy_na=True)\n",
    "    \n",
    "    # Handle infinite values\n",
    "    df_fe.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_fe.fillna(df_fe.median(), inplace=True)\n",
    "    \n",
    "    return df_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(X, y, n_trials=10, random_state=42):\n",
    "    \"\"\"Optimize XGBoost hyperparameters.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 4,\n",
    "            'use_label_encoder': False,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'random_state': random_state,\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_val = model.predict(X_val_fold)\n",
    "            f1_scores.append(f1_score(y_val_fold, y_pred_val, average='macro'))\n",
    "        \n",
    "        return np.mean(f1_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "def optimize_lightgbm(X, y, n_trials=10, random_state=42):\n",
    "    \"\"\"Optimize LightGBM hyperparameters.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': 4,\n",
    "            'random_state': random_state,\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        }\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_val = model.predict(X_val_fold)\n",
    "            f1_scores.append(f1_score(y_val_fold, y_pred_val, average='macro'))\n",
    "        \n",
    "        return np.mean(f1_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "def optimize_catboost(X, y, n_trials=10, random_state=42):\n",
    "    \"\"\"Optimize CatBoost hyperparameters.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'loss_function': 'MultiClass',\n",
    "            'verbose': 0,\n",
    "            'random_seed': random_state,\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'depth': trial.suggest_int('depth', 3, 10),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "            'bootstrap_type': 'Bernoulli'\n",
    "        }\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_val = model.predict(X_val_fold)\n",
    "            f1_scores.append(f1_score(y_val_fold, y_pred_val, average='macro'))\n",
    "        \n",
    "        return np.mean(f1_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble(X_train, y_train, best_xgb_params, best_lgb_params, best_cat_params):\n",
    "    \"\"\"Create and train ensemble model.\"\"\"\n",
    "    xgb_model = xgb.XGBClassifier(**best_xgb_params)\n",
    "    lgb_model = lgb.LGBMClassifier(**best_lgb_params)\n",
    "    cat_model = CatBoostClassifier(**best_cat_params)\n",
    "    \n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('xgb', xgb_model),\n",
    "            ('lgb', lgb_model),\n",
    "            ('cat', cat_model)\n",
    "        ],\n",
    "        voting='soft',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    return voting_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "download_kaggle_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training data\n",
    "df = load_and_preprocess_data('task2/train.csv', is_training=True)\n",
    "df_fe = engineer_features(df)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_fe.drop(columns=['segment'])\n",
    "y = df_fe['segment']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE for class balancing\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Training data shape: {X_train_res.shape}\")\n",
    "print(f\"Class distribution after SMOTE: {pd.Series(y_train_res).value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing XGBoost...\")\n",
    "best_xgb_params = optimize_xgboost(X_train_res, y_train_res, N_TRIALS, RANDOM_STATE)\n",
    "print(f\"Best XGBoost params: {best_xgb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing LightGBM...\")\n",
    "best_lgb_params = optimize_lightgbm(X_train_res, y_train_res, N_TRIALS, RANDOM_STATE)\n",
    "print(f\"Best LightGBM params: {best_lgb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing CatBoost...\")\n",
    "best_cat_params = optimize_catboost(X_train_res, y_train_res, N_TRIALS, RANDOM_STATE)\n",
    "print(f\"Best CatBoost params: {best_cat_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble\n",
    "voting_clf = create_ensemble(X_train_res, y_train_res, best_xgb_params, best_lgb_params, best_cat_params)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and preprocessors\n",
    "joblib.dump(voting_clf, 'voting_clf.joblib')\n",
    "joblib.dump(X_train_res.columns.tolist(), 'feature_columns.joblib')\n",
    "\n",
    "if COLAB_ENV:\n",
    "    files.download('voting_clf.joblib')\n",
    "    files.download('feature_columns.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_path, feature_columns):\n",
    "    \"\"\"Preprocess test data to match training format.\"\"\"\n",
    "    test_df, test_ids = load_and_preprocess_data(test_path, is_training=False)\n",
    "    test_df_fe = engineer_features(test_df)\n",
    "    \n",
    "    # Ensure test features match training features\n",
    "    missing_cols = set(feature_columns) - set(test_df_fe.columns)\n",
    "    for col in missing_cols:\n",
    "        test_df_fe[col] = 0\n",
    "    \n",
    "    # Align column order\n",
    "    test_df_fe = test_df_fe[feature_columns]\n",
    "    \n",
    "    return test_df_fe, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models and feature columns\n",
    "voting_clf = joblib.load('voting_clf.joblib')\n",
    "feature_columns = joblib.load('feature_columns.joblib')\n",
    "\n",
    "# Preprocess test data\n",
    "test_df_processed, test_ids = preprocess_test_data('task2/test.csv', feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions and Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = voting_clf.predict(test_df_processed)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'segment': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission created with {len(submission)} predictions\")\n",
    "print(f\"Prediction distribution: {submission['segment'].value_counts().to_dict()}\")\n",
    "\n",
    "if COLAB_ENV:\n",
    "    files.download('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
