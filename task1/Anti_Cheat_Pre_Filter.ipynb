{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti-Cheat Pre-Filter System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle optuna catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    COLAB_ENV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CORRELATION_THRESHOLD = 0.8\n",
    "RANDOM_STATE = 42\n",
    "N_TRIALS = 200\n",
    "KNN_COLS = ['kill_death_ratio', 'headshot_percentage', 'accuracy_score', 'damage_per_round']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_data():\n",
    "    \"\"\"Download and extract Kaggle competition data.\"\"\"\n",
    "    if COLAB_ENV:\n",
    "        try:\n",
    "            uploaded = files.upload()\n",
    "        except Exception as e:\n",
    "            print(f\"File upload failed: {e}\")\n",
    "    else:\n",
    "        print(\"Running outside of Colab. Ensure kaggle.json is in ~/.kaggle/\")\n",
    "\n",
    "    if 'kaggle.json' in os.listdir('.'):\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !mv kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "    else:\n",
    "        print(\"kaggle.json not found.\")\n",
    "\n",
    "    if not os.path.exists('cpe342-karena.zip'):\n",
    "        print(\"Downloading data...\")\n",
    "        !kaggle competitions download -c cpe342-karena\n",
    "    else:\n",
    "        print(\"Data already downloaded.\")\n",
    "\n",
    "    if os.path.exists('cpe342-karena.zip'):\n",
    "        print(\"Extracting data...\")\n",
    "        with zipfile.ZipFile('cpe342-karena.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(\"Data extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, knn_cols, knn_imputer=None, median_imputer=None, is_training=True):\n",
    "    \"\"\"Clean and impute missing values.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if is_training:\n",
    "        df_clean = df_clean.drop(['id', 'player_id'], axis=1, errors='ignore')\n",
    "        knn_imputer = KNNImputer(n_neighbors=5)\n",
    "        median_imputer = SimpleImputer(strategy='median')\n",
    "        \n",
    "        df_clean[knn_cols] = knn_imputer.fit_transform(df_clean[knn_cols])\n",
    "        \n",
    "        remaining_cols = df_clean.columns[df_clean.isnull().any()].tolist()\n",
    "        if 'is_cheater' in remaining_cols:\n",
    "            remaining_cols.remove('is_cheater')\n",
    "        \n",
    "        if remaining_cols:\n",
    "            df_clean[remaining_cols] = median_imputer.fit_transform(df_clean[remaining_cols])\n",
    "        \n",
    "        if 'is_cheater' in df_clean.columns:\n",
    "            df_clean = df_clean[df_clean['is_cheater'].notna()].reset_index(drop=True)\n",
    "    else:\n",
    "        df_clean[knn_cols] = knn_imputer.transform(df_clean[knn_cols])\n",
    "        remaining_cols = df_clean.columns[df_clean.isnull().any()].tolist()\n",
    "        if 'is_cheater' in remaining_cols:\n",
    "            remaining_cols.remove('is_cheater')\n",
    "        if remaining_cols:\n",
    "            df_clean[remaining_cols] = median_imputer.transform(df_clean[remaining_cols])\n",
    "    \n",
    "    return df_clean, knn_imputer, median_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Create new features from existing ones.\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Efficiency features\n",
    "    df_features[\"kill_efficiency\"] = df_features[\"kill_death_ratio\"] * df_features[\"accuracy_score\"]\n",
    "    df_features[\"headshot_ratio_to_accuracy\"] = df_features[\"headshot_percentage\"] / (df_features[\"accuracy_score\"] + 1e-6)\n",
    "    df_features[\"reaction_accuracy_ratio\"] = df_features[\"accuracy_score\"] / (df_features[\"reaction_time_ms\"] + 1e-6)\n",
    "    df_features[\"damage_efficiency\"] = df_features[\"damage_per_round\"] / (df_features[\"survival_time_avg\"] + 1e-6)\n",
    "    \n",
    "    # Stability features\n",
    "    df_features[\"reports_per_day\"] = df_features[\"reports_received\"] / (df_features[\"account_age_days\"] + 1)\n",
    "    df_features[\"device_change_rate\"] = df_features[\"device_changes_count\"] / (df_features[\"account_age_days\"] + 1)\n",
    "    df_features[\"session_intensity\"] = df_features[\"sessions_per_day\"] * df_features[\"avg_session_length_min\"]\n",
    "    \n",
    "    # Behavioral features\n",
    "    df_features[\"performance_per_account_age\"] = (\n",
    "        (df_features[\"kill_death_ratio\"] + df_features[\"accuracy_score\"] + df_features[\"win_rate\"]) / \n",
    "        (df_features[\"account_age_days\"] + 1)\n",
    "    )\n",
    "    df_features[\"input_to_accuracy_ratio\"] = df_features[\"input_consistency_score\"] / (df_features[\"accuracy_score\"] + 1e-6)\n",
    "    df_features[\"friendliness_ratio\"] = (\n",
    "        df_features[\"communication_rate\"] * df_features[\"team_play_score\"] / \n",
    "        (df_features[\"reports_per_day\"] + 1e-6)\n",
    "    )\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(df, scaler=None, is_training=True):\n",
    "    \"\"\"Standardize numerical features.\"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    exclude_cols = ['id', 'player_id', 'is_cheater']\n",
    "    \n",
    "    numerical_cols = df_scaled.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
    "    \n",
    "    if is_training:\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled[numerical_cols] = scaler.fit_transform(df_scaled[numerical_cols])\n",
    "    else:\n",
    "        df_scaled[numerical_cols] = scaler.transform(df_scaled[numerical_cols])\n",
    "    \n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_features(df, threshold=0.9):\n",
    "    \"\"\"Remove highly correlated features.\"\"\"\n",
    "    df_reduced = df.copy()\n",
    "    exclude_cols = ['id', 'player_id', 'is_cheater']\n",
    "    \n",
    "    numerical_cols = df_reduced.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
    "    \n",
    "    corr_matrix = df_reduced[numerical_cols].corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    features_to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    df_reduced.drop(columns=features_to_drop, inplace=True)\n",
    "    \n",
    "    return df_reduced, features_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_catboost(X, y, n_trials=50, random_state=42):\n",
    "    \"\"\"Optimize CatBoost hyperparameters.\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 200, 800),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 12),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 10.0, log=True),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 5.0),\n",
    "            \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "            \"loss_function\": \"Logloss\",\n",
    "            \"eval_metric\": \"Logloss\",\n",
    "            \"random_seed\": random_state,\n",
    "            \"verbose\": False,\n",
    "            \"class_weights\": class_weight_dict,\n",
    "        }\n",
    "        \n",
    "        threshold = trial.suggest_float(\"threshold\", 0.05, 0.80)\n",
    "        model = cb.CatBoostClassifier(**params)\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "        f2_scores = []\n",
    "        \n",
    "        for train_idx, valid_idx in kf.split(X, y):\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            preds = model.predict_proba(X.iloc[valid_idx])[:, 1]\n",
    "            preds_bin = (preds > threshold).astype(int)\n",
    "            f2 = fbeta_score(y.iloc[valid_idx], preds_bin, beta=2)\n",
    "            f2_scores.append(f2)\n",
    "        \n",
    "        return np.mean(f2_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(X, y, n_trials=50, random_state=42):\n",
    "    \"\"\"Optimize XGBoost hyperparameters.\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
    "    pos_weight = class_weights[1] / class_weights[0]\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1, 20),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "            \"random_state\": random_state,\n",
    "            \"scale_pos_weight\": pos_weight,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"eval_metric\": \"logloss\"\n",
    "        }\n",
    "        \n",
    "        threshold = trial.suggest_float(\"threshold\", 0.05, 0.80)\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "        f2_scores = []\n",
    "        \n",
    "        for train_idx, valid_idx in kf.split(X, y):\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            preds = model.predict_proba(X.iloc[valid_idx])[:, 1]\n",
    "            preds_bin = (preds > threshold).astype(int)\n",
    "            f2 = fbeta_score(y.iloc[valid_idx], preds_bin, beta=2)\n",
    "            f2_scores.append(f2)\n",
    "        \n",
    "        return np.mean(f2_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lightgbm(X, y, n_trials=50, random_state=42):\n",
    "    \"\"\"Optimize LightGBM hyperparameters.\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", -1, 12),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "            \"random_state\": random_state,\n",
    "            \"class_weight\": class_weight_dict,\n",
    "        }\n",
    "        \n",
    "        threshold = trial.suggest_float(\"threshold\", 0.05, 0.80)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "        f2_scores = []\n",
    "        \n",
    "        for train_idx, valid_idx in kf.split(X, y):\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            preds = model.predict_proba(X.iloc[valid_idx])[:, 1]\n",
    "            preds_bin = (preds > threshold).astype(int)\n",
    "            f2 = fbeta_score(y.iloc[valid_idx], preds_bin, beta=2)\n",
    "            f2_scores.append(f2)\n",
    "        \n",
    "        return np.mean(f2_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble(X_train, y_train, best_cb, best_xgb, best_lgb):\n",
    "    \"\"\"Create ensemble models.\"\"\"\n",
    "    threshold_cb = best_cb.pop(\"threshold\")\n",
    "    threshold_xgb = best_xgb.pop(\"threshold\")\n",
    "    threshold_lgb = best_lgb.pop(\"threshold\")\n",
    "    \n",
    "    model_cb = cb.CatBoostClassifier(verbose=False, **best_cb)\n",
    "    model_xgb = xgb.XGBClassifier(**best_xgb)\n",
    "    model_lgb = lgb.LGBMClassifier(**best_lgb)\n",
    "    \n",
    "    model_cb.fit(X_train, y_train)\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    model_lgb.fit(X_train, y_train)\n",
    "    \n",
    "    return {\n",
    "        \"cb\": model_cb,\n",
    "        \"xgb\": model_xgb,\n",
    "        \"lgb\": model_lgb,\n",
    "        \"thresholds\": {\n",
    "            \"cb\": threshold_cb,\n",
    "            \"xgb\": threshold_xgb,\n",
    "            \"lgb\": threshold_lgb\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "download_kaggle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize imputers (will be fitted during training)\n",
    "knn_imputer = None\n",
    "median_imputer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv('public_dataset/task1/train.csv')\n",
    "df = df[df['is_cheater'].notna()].reset_index(drop=True)\n",
    "\n",
    "df_clean, knn_imputer, median_imputer = clean_data(df, KNN_COLS, knn_imputer, median_imputer, is_training=True)\n",
    "df_feat = engineer_features(df_clean)\n",
    "df_scaled, scaler = standardize_features(df_feat, is_training=True)\n",
    "df_final, dropped_features = remove_correlated_features(df_scaled, threshold=CORRELATION_THRESHOLD)\n",
    "\n",
    "X_train = df_final.drop(columns=['is_cheater'])\n",
    "y_train = df_final['is_cheater']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing CatBoost...\")\n",
    "best_cb = optimize_catboost(X_train, y_train, N_TRIALS, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing XGBoost...\")\n",
    "best_xgb = optimize_xgboost(X_train, y_train, N_TRIALS, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing LightGBM...\")\n",
    "best_lgb = optimize_lightgbm(X_train, y_train, N_TRIALS, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble\n",
    "ensemble = create_ensemble(X_train, y_train, best_cb, best_xgb, best_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and preprocessors\n",
    "joblib.dump(ensemble, 'ensemble.joblib')\n",
    "joblib.dump(knn_imputer, 'knn_imputer.joblib')\n",
    "joblib.dump(median_imputer, 'median_imputer.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(dropped_features, 'dropped_features.joblib')\n",
    "\n",
    "if COLAB_ENV:\n",
    "    files.download('ensemble.joblib')\n",
    "    files.download('knn_imputer.joblib')\n",
    "    files.download('median_imputer.joblib')\n",
    "    files.download('scaler.joblib')\n",
    "    files.download('dropped_features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_path, knn_imputer, median_imputer, scaler, dropped_features):\n",
    "    \"\"\"Preprocess test data using fitted transformers.\"\"\"\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    test_ids = df_test['id'].copy()\n",
    "    \n",
    "    # Clean and transform test data\n",
    "    df_test_clean, _, _ = clean_data(df_test, KNN_COLS, knn_imputer, median_imputer, is_training=False)\n",
    "    df_test_feat = engineer_features(df_test_clean)\n",
    "    df_test_scaled, _ = standardize_features(df_test_feat, scaler=scaler, is_training=False)\n",
    "    \n",
    "    # Remove dropped features\n",
    "    X_test = df_test_scaled.drop(columns=[f for f in dropped_features if f in df_test_scaled.columns])\n",
    "    \n",
    "    return X_test, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble_predictions(ensemble, X_test):\n",
    "    \"\"\"Make predictions using ensemble with optimized thresholds.\"\"\"\n",
    "    # Get predictions from each model\n",
    "    pred_cb = ensemble['cb'].predict_proba(X_test)[:, 1]\n",
    "    pred_xgb = ensemble['xgb'].predict_proba(X_test)[:, 1]\n",
    "    pred_lgb = ensemble['lgb'].predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Apply optimized thresholds\n",
    "    pred_cb_bin = (pred_cb > ensemble['thresholds']['cb']).astype(int)\n",
    "    pred_xgb_bin = (pred_xgb > ensemble['thresholds']['xgb']).astype(int)\n",
    "    pred_lgb_bin = (pred_lgb > ensemble['thresholds']['lgb']).astype(int)\n",
    "    \n",
    "    # Majority voting\n",
    "    ensemble_pred = np.round((pred_cb_bin + pred_xgb_bin + pred_lgb_bin) / 3).astype(int)\n",
    "    \n",
    "    return ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models and preprocessors\n",
    "ensemble = joblib.load('ensemble.joblib')\n",
    "knn_imputer = joblib.load('knn_imputer.joblib')\n",
    "median_imputer = joblib.load('median_imputer.joblib')\n",
    "scaler = joblib.load('scaler.joblib')\n",
    "dropped_features = joblib.load('dropped_features.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "X_test, test_ids = preprocess_test_data(\n",
    "    'public_dataset/task1/test.csv', \n",
    "    knn_imputer, median_imputer, scaler, dropped_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions and Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = make_ensemble_predictions(ensemble, X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'task1': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission created with {len(submission)} predictions\")\n",
    "print(f\"Prediction distribution: {submission['task1'].value_counts().to_dict()}\")\n",
    "\n",
    "if COLAB_ENV:\n",
    "    files.download('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
