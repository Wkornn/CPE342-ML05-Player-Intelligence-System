{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player Monthly Spending Prediction - Refactored\n",
    "\n",
    "Clean, standardized implementation of the player spending prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle optuna catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "import optuna\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    COLAB_ENV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "N_TRIALS = 40\n",
    "TEST_SIZE = 0.15\n",
    "CV_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_data():\n",
    "    \"\"\"Download and extract Kaggle competition data.\"\"\"\n",
    "    if COLAB_ENV:\n",
    "        try:\n",
    "            uploaded = files.upload()\n",
    "        except Exception as e:\n",
    "            print(f\"File upload failed: {e}\")\n",
    "    else:\n",
    "        print(\"Running outside of Colab. Ensure kaggle.json is in ~/.kaggle/\")\n",
    "\n",
    "    if 'kaggle.json' in os.listdir('.'):\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !mv kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "    else:\n",
    "        print(\"kaggle.json not found.\")\n",
    "\n",
    "    if not os.path.exists('cpe342-karena.zip'):\n",
    "        print(\"Downloading data...\")\n",
    "        !kaggle competitions download -c cpe342-karena\n",
    "    else:\n",
    "        print(\"Data already downloaded.\")\n",
    "\n",
    "    if os.path.exists('cpe342-karena.zip'):\n",
    "        print(\"Extracting data...\")\n",
    "        with zipfile.ZipFile('cpe342-karena.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(\"Data extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, is_training=True):\n",
    "    \"\"\"Load and preprocess player spending data.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if is_training:\n",
    "        df = df.drop(['id', 'player_id'], axis=1, errors='ignore')\n",
    "        # Create binary target and log target\n",
    "        df['will_spend'] = (df['spending_30d'] > 0).astype(int)\n",
    "        df['log_spend'] = np.log1p(df['spending_30d'])\n",
    "    else:\n",
    "        ids = df['id'].copy()\n",
    "        df = df.drop(['id', 'player_id'], axis=1, errors='ignore')\n",
    "        return df, ids\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor():\n",
    "    \"\"\"Create preprocessing pipeline for features.\"\"\"\n",
    "    # Identify feature types\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='__missing__')),\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "    \n",
    "    # Note: Features will be determined dynamically based on data types\n",
    "    return numeric_transformer, categorical_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_classifier(X, y, preprocessor, n_trials=30, random_state=42):\n",
    "    \"\"\"Optimize CatBoost classifier hyperparameters.\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 200, 1500),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
    "            'random_seed': random_state,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        \n",
    "        clf = Pipeline([\n",
    "            ('preproc', preprocessor),\n",
    "            ('clf', CatBoostClassifier(**params))\n",
    "        ])\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        p = clf.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        from sklearn.metrics import log_loss\n",
    "        return log_loss(y_val, p, labels=[0, 1])\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_regressor(X, y, preprocessor, n_trials=40, random_state=42):\n",
    "    \"\"\"Optimize LightGBM regressor hyperparameters.\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 2000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),\n",
    "            'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "            'random_state': random_state\n",
    "        }\n",
    "        \n",
    "        reg = Pipeline([\n",
    "            ('preproc', preprocessor),\n",
    "            ('reg', lgb.LGBMRegressor(**params))\n",
    "        ])\n",
    "        \n",
    "        reg.fit(X_train, y_train)\n",
    "        pred = np.expm1(reg.predict(X_val))\n",
    "        true = np.expm1(y_val)\n",
    "        \n",
    "        return mean_absolute_error(true, pred)\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_two_stage_model(X_train, y_cls, y_spend, best_clf_params, best_reg_params, preprocessor):\n",
    "    \"\"\"Create and train two-stage model.\"\"\"\n",
    "    # Stage 1: Classifier\n",
    "    clf = Pipeline([\n",
    "        ('preproc', preprocessor),\n",
    "        ('clf', CatBoostClassifier(verbose=0, random_seed=RANDOM_STATE, **best_clf_params))\n",
    "    ])\n",
    "    clf.fit(X_train, y_cls)\n",
    "    \n",
    "    # Stage 2: Regressor (train only on positive spenders)\n",
    "    pos_mask = y_spend > 0\n",
    "    X_train_reg = X_train[pos_mask]\n",
    "    y_train_reg = np.log1p(y_spend[pos_mask])\n",
    "    \n",
    "    reg = Pipeline([\n",
    "        ('preproc', preprocessor),\n",
    "        ('reg', lgb.LGBMRegressor(random_state=RANDOM_STATE, **best_reg_params))\n",
    "    ])\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    return clf, reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "download_kaggle_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training data\n",
    "df = load_and_preprocess_data('task3/train.csv', is_training=True)\n",
    "\n",
    "# Separate features and targets\n",
    "feature_cols = [c for c in df.columns if c not in ['spending_30d', 'will_spend', 'log_spend']]\n",
    "X = df[feature_cols].copy()\n",
    "y_cls = df['will_spend']\n",
    "y_spend = df['spending_30d']\n",
    "\n",
    "# Identify feature types\n",
    "num_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {len(num_features)}\")\n",
    "print(f\"Categorical features: {len(cat_features)}\")\n",
    "\n",
    "# Create preprocessor\n",
    "numeric_transformer, categorical_transformer = create_preprocessor()\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, num_features),\n",
    "    ('cat', categorical_transformer, cat_features)\n",
    "], remainder='drop', sparse_threshold=0)\n",
    "\n",
    "# Split data\n",
    "X_train, X_holdout, y_train_cls, y_holdout_cls, y_train_spend, y_holdout_spend = train_test_split(\n",
    "    X, y_cls, y_spend, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_cls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing classifier...\")\n",
    "best_clf_params = optimize_classifier(X_train, y_train_cls, preprocessor, N_TRIALS, RANDOM_STATE)\n",
    "print(f\"Best classifier params: {best_clf_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing regressor...\")\n",
    "# Prepare regressor training data (positive spenders only)\n",
    "pos_mask = y_train_spend > 0\n",
    "X_train_reg = X_train[pos_mask]\n",
    "y_train_reg = np.log1p(y_train_spend[pos_mask])\n",
    "\n",
    "best_reg_params = optimize_regressor(X_train_reg, y_train_reg, preprocessor, N_TRIALS, RANDOM_STATE)\n",
    "print(f\"Best regressor params: {best_reg_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final models\n",
    "clf_final, reg_final = create_two_stage_model(\n",
    "    X_train, y_train_cls, y_train_spend, \n",
    "    best_clf_params, best_reg_params, preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on holdout\n",
    "p_holdout = clf_final.predict_proba(X_holdout)[:, 1]\n",
    "pred_log = reg_final.predict(X_holdout)\n",
    "pred_spend = np.expm1(pred_log)\n",
    "final_pred = p_holdout * pred_spend\n",
    "\n",
    "mae = mean_absolute_error(y_holdout_spend, final_pred)\n",
    "normalized_mae = mae / (y_holdout_spend.mean() + 1e-9)\n",
    "\n",
    "print(f\"Holdout MAE: {mae:.4f}\")\n",
    "print(f\"Holdout Normalized MAE: {normalized_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "joblib.dump(clf_final, 'classifier.joblib')\n",
    "joblib.dump(reg_final, 'regressor.joblib')\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "joblib.dump(feature_cols, 'feature_cols.joblib')\n",
    "\n",
    "if COLAB_ENV:\n",
    "    files.download('classifier.joblib')\n",
    "    files.download('regressor.joblib')\n",
    "    files.download('preprocessor.joblib')\n",
    "    files.download('feature_cols.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_path, feature_cols):\n",
    "    \"\"\"Preprocess test data using saved feature columns.\"\"\"\n",
    "    df_test, test_ids = load_and_preprocess_data(test_path, is_training=False)\n",
    "    \n",
    "    # Ensure test data has same features as training\n",
    "    X_test = df_test[feature_cols].copy()\n",
    "    \n",
    "    return X_test, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_two_stage_predictions(clf, reg, X_test):\n",
    "    \"\"\"Make predictions using two-stage model.\"\"\"\n",
    "    # Stage 1: Probability of spending\n",
    "    p_spend = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Stage 2: Amount prediction (log scale)\n",
    "    pred_log = reg.predict(X_test)\n",
    "    pred_amount = np.expm1(pred_log)\n",
    "    \n",
    "    # Final prediction: probability * amount\n",
    "    final_pred = p_spend * pred_amount\n",
    "    \n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models\n",
    "clf_final = joblib.load('classifier.joblib')\n",
    "reg_final = joblib.load('regressor.joblib')\n",
    "feature_cols = joblib.load('feature_cols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "X_test, test_ids = preprocess_test_data('task3/test.csv', feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions and Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = make_two_stage_predictions(clf_final, reg_final, X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'task3': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Submission created with {len(submission)} predictions\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"  Mean: {predictions.mean():.2f}\")\n",
    "print(f\"  Median: {np.median(predictions):.2f}\")\n",
    "print(f\"  Max: {predictions.max():.2f}\")\n",
    "print(f\"  Zero predictions: {(predictions == 0).sum()}\")\n",
    "\n",
    "if COLAB_ENV:\n",
    "    files.download('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}