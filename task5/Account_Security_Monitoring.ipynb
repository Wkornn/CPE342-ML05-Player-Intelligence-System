{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Account Security Monitoring - Refactored\n",
        "\n",
        "Clean, standardized implementation of the anomaly detection pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "RANDOM_STATE = 42\n",
        "CONTAMINATION_RATE = 0.15\n",
        "PCA_VARIANCE_RATIO = 0.8\n",
        "ANOMALY_THRESHOLD_PERCENTILE = 85\n",
        "N_ESTIMATORS = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_kaggle_data():\n",
        "    \"\"\"Download and extract Kaggle competition data.\"\"\"\n",
        "    if COLAB_ENV:\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "        except Exception as e:\n",
        "            print(f\"File upload failed: {e}\")\n",
        "    else:\n",
        "        print(\"Running outside of Colab. Ensure kaggle.json is in ~/.kaggle/\")\n",
        "\n",
        "    if 'kaggle.json' in os.listdir('.'):\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !mv kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "    else:\n",
        "        print(\"kaggle.json not found.\")\n",
        "\n",
        "    if not os.path.exists('cpe342-karena.zip'):\n",
        "        print(\"Downloading data...\")\n",
        "        !kaggle competitions download -c cpe342-karena\n",
        "    else:\n",
        "        print(\"Data already downloaded.\")\n",
        "\n",
        "    if os.path.exists('cpe342-karena.zip'):\n",
        "        print(\"Extracting data...\")\n",
        "        with zipfile.ZipFile('cpe342-karena.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "        print(\"Data extracted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_base_features(df):\n",
        "    \"\"\"Extract base feature names from time-series columns.\"\"\"\n",
        "    base_features = set()\n",
        "    for col in df.columns:\n",
        "        if col.endswith('_1'):\n",
        "            base_features.add(col[:-2])\n",
        "    return base_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_aggregated_features(df):\n",
        "    \"\"\"Create aggregated features from time-series data.\"\"\"\n",
        "    base_features = extract_base_features(df)\n",
        "    X_eng = df.copy()\n",
        "    \n",
        "    for base in base_features:\n",
        "        cols = [f\"{base}_{i}\" for i in range(1, 5)]\n",
        "        \n",
        "        if all(c in df.columns for c in cols):\n",
        "            X_eng[f'{base}_mean_agg'] = df[cols].mean(axis=1)\n",
        "            X_eng[f'{base}_std_agg'] = df[cols].std(axis=1)\n",
        "            X_eng[f'{base}_range_agg'] = df[cols].max(axis=1) - df[cols].min(axis=1)\n",
        "    \n",
        "    return X_eng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess data with feature engineering and scaling.\"\"\"\n",
        "    X_eng = create_aggregated_features(df)\n",
        "    X = X_eng.select_dtypes(include=[np.number])\n",
        "    \n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "    \n",
        "    return X_scaled, imputer, scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anomaly Detection Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_isolation_forest(X_scaled):\n",
        "    \"\"\"Train Isolation Forest model.\"\"\"\n",
        "    iso = IsolationForest(\n",
        "        n_estimators=N_ESTIMATORS,\n",
        "        contamination=CONTAMINATION_RATE,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    iso.fit(X_scaled)\n",
        "    return iso.decision_function(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_pca_reconstruction(X_scaled):\n",
        "    \"\"\"Train PCA reconstruction model.\"\"\"\n",
        "    n_components = int(X_scaled.shape[1] * PCA_VARIANCE_RATIO)\n",
        "    pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
        "    \n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "    \n",
        "    reconstruction_error = np.mean(np.square(X_scaled - X_reconstructed), axis=1)\n",
        "    return reconstruction_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combine_anomaly_scores(iso_scores, reconstruction_error):\n",
        "    \"\"\"Combine anomaly scores from multiple models.\"\"\"\n",
        "    scaler_mm = MinMaxScaler()\n",
        "    \n",
        "    # Normalize Isolation Forest scores (invert: 0=bad, 1=good -> 1=bad, 0=good)\n",
        "    s1 = scaler_mm.fit_transform(iso_scores.reshape(-1, 1)).flatten()\n",
        "    s1 = 1 - s1\n",
        "    \n",
        "    # Normalize PCA reconstruction error (1=bad, 0=good)\n",
        "    s2 = scaler_mm.fit_transform(reconstruction_error.reshape(-1, 1)).flatten()\n",
        "    \n",
        "    # Average scores\n",
        "    return (s1 + s2) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_anomalies(final_score, threshold_percentile=ANOMALY_THRESHOLD_PERCENTILE):\n",
        "    \"\"\"Predict anomalies based on threshold.\"\"\"\n",
        "    threshold_val = np.percentile(final_score, threshold_percentile)\n",
        "    return [1 if x >= threshold_val else 0 for x in final_score]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data\n",
        "download_kaggle_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "try:\n",
        "    df = pd.read_csv('public_dataset/task5/test.csv')\n",
        "    print(\"Data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: test.csv not found.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "X_scaled, imputer, scaler = preprocess_data(df)\n",
        "print(f\"Preprocessed data shape: {X_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Isolation Forest\n",
        "print(\"Training Isolation Forest...\")\n",
        "iso_scores = train_isolation_forest(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train PCA Reconstruction\n",
        "print(\"Training PCA Reconstruction...\")\n",
        "reconstruction_error = train_pca_reconstruction(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine scores and predict\n",
        "final_score = combine_anomaly_scores(iso_scores, reconstruction_error)\n",
        "final_predictions = predict_anomalies(final_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create submission\n",
        "submission = df.copy()\n",
        "submission['is_anomaly'] = final_predictions\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "anomaly_count = sum(final_predictions)\n",
        "anomaly_rate = anomaly_count / len(final_predictions) * 100\n",
        "\n",
        "print(f\"Submission created with ensemble approach.\")\n",
        "print(f\"Total anomalies flagged: {anomaly_count} ({anomaly_rate:.2f}%)\")\n",
        "\n",
        "if COLAB_ENV:\n",
        "    files.download('submission.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}